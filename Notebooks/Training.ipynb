{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:15:27.764438Z",
     "start_time": "2020-12-21T13:15:24.463408Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "import pickle\n",
    "from transformers import Trainer,TrainingArguments,BartConfig,BartTokenizer,BartModel,BartForConditionalGeneration,BartTokenizerFast\n",
    "from torch.utils import data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"big_data\" contains precomputed simulations for 198 length sequences.\n",
    "\n",
    "Do not recommend for CPU systems, or if you want to do this quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:15:27.768344Z",
     "start_time": "2020-12-21T13:15:27.765414Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"../DataSets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:15:28.008103Z",
     "start_time": "2020-12-21T13:15:28.005174Z"
    }
   },
   "outputs": [],
   "source": [
    "big_data = data_dir+\"dna_big_sim_output.txt\"\n",
    "big_data_header = [\"dna\",\"length\",\"energy\",\"struct\",\"blank\",\"prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"small_data\" is only of length 32, with fewer sequences and is more managable\n",
    "\n",
    "Future work will be on a more flexible model that can handle variable lengths, but current intended use is only on sequences of a known set length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:15:28.834842Z",
     "start_time": "2020-12-21T13:15:28.831913Z"
    }
   },
   "outputs": [],
   "source": [
    "small_data = data_dir+\"dna_small_sim_output.txt\"\n",
    "small_data_header = [\"dna\",\"length\",\"energy\",\"struct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load prefered data set into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:15:29.972622Z",
     "start_time": "2020-12-21T13:15:29.647903Z"
    }
   },
   "outputs": [],
   "source": [
    "header_names = small_data_header\n",
    "data = pd.read_csv(small_data,sep=\"\\\\t\",header=None,names=header_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the DNA and structure up for the tokenizer \n",
    "\n",
    "This is because the tokenizer expects to see words, not individual characters \n",
    "\n",
    "(it was made originally for NLP)\n",
    "\n",
    "(building custom tokenizer may be possible, but model was prebuilt to be used with this tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:15:33.323065Z",
     "start_time": "2020-12-21T13:15:33.169748Z"
    }
   },
   "outputs": [],
   "source": [
    "dna = data[\"dna\"]\n",
    "struct = data[\"struct\"]\n",
    "\n",
    "dna_list = [\" \".join(list(d)) for d in data[\"dna\"]]\n",
    "struct_list = [\" \".join(list(s)) for s in data[\"struct\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into a train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:15:33.980818Z",
     "start_time": "2020-12-21T13:15:33.977889Z"
    }
   },
   "outputs": [],
   "source": [
    "length = len(dna) # total number of elements (this was used in the test version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:15:34.580182Z",
     "start_time": "2020-12-21T13:15:34.577251Z"
    }
   },
   "outputs": [],
   "source": [
    "# run this block for an easier, shorter run \n",
    "length = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:18:31.246405Z",
     "start_time": "2020-12-21T13:18:31.243324Z"
    }
   },
   "outputs": [],
   "source": [
    "split = int(length * 0.75) # default for this is 75/25 train-val.\n",
    "\n",
    "dna_train = dna_list[:split]\n",
    "dna_val = dna_list[split:length-1]\n",
    "\n",
    "struct_train = struct_list[:split]\n",
    "struct_val = struct_list[split:length-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a prebuilt tokenizer that is known to work with this model\n",
    "\n",
    "NOTE: may take a while on first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:15:48.942162Z",
     "start_time": "2020-12-21T13:15:46.887360Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:18:41.129450Z",
     "start_time": "2020-12-21T13:18:40.745164Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer.prepare_seq2seq_batch(src_texts = dna_train, \n",
    "                                                  tgt_texts = struct_train,\n",
    "                                                  padding=True,\n",
    "                                                  return_tensors='pt',\n",
    "                                                  truncation=True,\n",
    "                                                  #return_token_type_ids = True,\n",
    "                                                  max_length=len(dna[0])+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:18:41.274486Z",
     "start_time": "2020-12-21T13:18:41.152887Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_val = tokenizer.prepare_seq2seq_batch(src_texts = dna_val, \n",
    "                                                tgt_texts = struct_val,\n",
    "                                                padding=True,\n",
    "                                                return_tensors='pt',\n",
    "                                                truncation=True,\n",
    "                                                #return_token_type_ids = True,\n",
    "                                                max_length=len(dna[0])+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom dataset class because pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:16:32.461329Z",
     "start_time": "2020-12-21T13:16:32.457423Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(data_utils.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # self.encodings.keys() = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pyTorch Datasets for training (tds) and evalidation (eds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:18:42.736830Z",
     "start_time": "2020-12-21T13:18:42.733900Z"
    }
   },
   "outputs": [],
   "source": [
    "tds = MyDataset(tokenized_train)\n",
    "eds = MyDataset(tokenized_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments for the trainer\n",
    "\n",
    "Training can take a while before a good result is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:16:34.681547Z",
     "start_time": "2020-12-21T13:16:34.677641Z"
    }
   },
   "outputs": [],
   "source": [
    "small_training_args = TrainingArguments(\n",
    "    output_dir = \"./BART_Small_Output\", # where to store the checkpoints\n",
    "    logging_dir = \"./BART_Small_log\", # logging directory\n",
    "    per_device_train_batch_size=256,  # batch size per device during training NOTE: if on CPU or if you get OOM errors, set to smaller number\n",
    "    per_device_eval_batch_size=256,   # batch size for evaluation NOTE: if on CPU or if you get OOM errors, set to smaller number\n",
    "    fp16 = True, # if having NAN errors, disable. keeping true helps run faster\n",
    "    num_train_epochs=100) # number of training epochs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_training_args = TrainingArguments(\n",
    "    output_dir = \"./BART_Big_Output\", # where to store the checkpoints\n",
    "    logging_dir = \"./BART_Big_log\", # logging directory\n",
    "    per_device_train_batch_size=64,  # batch size per device during training NOTE: if on CPU or if you get OOM errors, set to smaller number\n",
    "    per_device_eval_batch_size=63,   # batch size for evaluation NOTE: if on CPU or if you get OOM errors, set to smaller number\n",
    "    fp16 = True, # if having NAN errors, disable. keeping true helps run faster\n",
    "    num_train_epochs=100) # number of training epochs to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:18:47.391623Z",
     "start_time": "2020-12-21T13:18:46.423404Z"
    }
   },
   "outputs": [],
   "source": [
    "config = BartConfig(\n",
    "    d_model = 256,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=6,\n",
    "    encoder_attention_heads=8,\n",
    "    decoder_attention_heads=8,\n",
    "    decoder_ffn_dim=1024,\n",
    "    encoder_ffn_dim=1024\n",
    "    )\n",
    "model = BartForConditionalGeneration (config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncommont and fill in URI for a previous training checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:18:47.432637Z",
     "start_time": "2020-12-21T13:18:47.429708Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = BartForConditionalGeneration.from_pretrained('.\\<Path To Model>', return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the trainer from previous arguments and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:18:48.549004Z",
     "start_time": "2020-12-21T13:18:48.515801Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model = model, \n",
    "                         args = small_training_args, \n",
    "                         train_dataset =tds,\n",
    "                         eval_dataset = eds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the trainer\n",
    "\n",
    "It will create checkpoints every 500 steps\n",
    "\n",
    "(recommend to clean out checkpoints, they can take up a lot of space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:26:59.480288Z",
     "start_time": "2020-12-21T13:18:50.401388Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:27:05.549703Z",
     "start_time": "2020-12-21T13:27:05.380761Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('DNA_BART_32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:38:40.272621Z",
     "start_time": "2020-12-20T21:38:40.264613Z"
    }
   },
   "source": [
    "Open the testing notebook to try out your model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
