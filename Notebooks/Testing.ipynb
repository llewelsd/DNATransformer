{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:28:56.667250Z",
     "start_time": "2020-12-21T13:28:56.653578Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "from transformers import Trainer,TrainingArguments,BartConfig,BartTokenizer,BartModel,BartForConditionalGeneration\n",
    "from transformers import BartTokenizerFast\n",
    "from torch.utils import data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:28:59.172730Z",
     "start_time": "2020-12-21T13:28:59.167846Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"../DataSets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"big_data\" contains precomputed simulations for 198 length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:29:00.610118Z",
     "start_time": "2020-12-21T13:29:00.606212Z"
    }
   },
   "outputs": [],
   "source": [
    "big_data = data_dir+\"dna_big_sim_output.txt\"\n",
    "big_data_header = [\"dna\",\"length\",\"energy\",\"struct\",\"blank\",\"prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T05:01:11.571715Z",
     "start_time": "2020-12-21T05:01:11.564879Z"
    }
   },
   "source": [
    "\"small_data\" is only of length 32, with fewer sequences and is more managable\n",
    "\n",
    "Future work will be on a more flexible model that can handle variable lengths, but current intended use is only on sequences of a known set length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:29:01.478301Z",
     "start_time": "2020-12-21T13:29:01.475372Z"
    }
   },
   "outputs": [],
   "source": [
    "small_data = data_dir+\"dna_small_sim_output.txt\"\n",
    "small_data_header = [\"dna\",\"length\",\"energy\",\"struct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load prefered data set into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:29:03.292729Z",
     "start_time": "2020-12-21T13:29:02.687743Z"
    }
   },
   "outputs": [],
   "source": [
    "header_names = small_data_header\n",
    "data = pd.read_csv(small_data,sep=\"\\\\t\",header=None,names=header_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the DNA and structure up for the tokenizer \n",
    "\n",
    "This is because the tokenizer expects to see words, not individual characters \n",
    "\n",
    "(it was made originally for NLP)\n",
    "\n",
    "(building custom tokenizer may be possible, but model was prebuilt to be used with this tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:29:05.524066Z",
     "start_time": "2020-12-21T13:29:05.360983Z"
    }
   },
   "outputs": [],
   "source": [
    "dna = data[\"dna\"]\n",
    "struct = data[\"struct\"]\n",
    "\n",
    "dna_list = [\" \".join(list(d)) for d in data[\"dna\"]]\n",
    "struct_list = [\" \".join(list(s)) for s in data[\"struct\"]]\n",
    "\n",
    "length=len(dna[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using same tokenizer model was trained for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:29:09.683991Z",
     "start_time": "2020-12-21T13:29:07.553792Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:44:17.341982Z",
     "start_time": "2020-12-20T21:44:17.326968Z"
    }
   },
   "source": [
    "Load model from trainer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:29:10.068749Z",
     "start_time": "2020-12-21T13:29:10.064843Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = \"../Models/\"\n",
    "\n",
    "small_model = model_dir + \"DNA_BART_32\"\n",
    "big_model = model_dir + \"DNA_BART_198\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:29:13.447942Z",
     "start_time": "2020-12-21T13:29:12.418498Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(small_model, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will translate individual sequences or batches of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:29:13.986994Z",
     "start_time": "2020-12-21T13:29:13.965510Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate(dna,model=model,tokenizer=tokenizer,max_length=length+2):#+2 is because models are weird\n",
    "    # tokenize input\n",
    "    inputTensor = tokenizer(dna,return_tensors='pt')\n",
    "    # generate output tensor\n",
    "    outputTensor = model.generate(inputTensor['input_ids'],\n",
    "                                 max_length = max_length,\n",
    "                                  early_stopping=True)\n",
    "    # decode and clean up output\n",
    "    output = [\"\".join(tokenizer.decode(t[2:-1].tolist()).split()) for t in outputTensor]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this test, compare the output from translating the first 1000 lines to a pre_run version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T13:40:40.441264Z",
     "start_time": "2020-12-21T13:40:06.233532Z"
    }
   },
   "outputs": [],
   "source": [
    "output = translate(dna_list[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T10:11:45.876631Z",
     "start_time": "2020-12-21T10:11:45.872725Z"
    }
   },
   "outputs": [],
   "source": [
    "val_file_path = data_dir + \"small_bart_output_1000.txt\"\n",
    "\n",
    "with open(val_file_path) as f:\n",
    "    val_output = f.readlines()\n",
    "val_output = [l.strip() for l in val_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T10:11:48.611649Z",
     "start_time": "2020-12-21T10:11:48.608720Z"
    }
   },
   "outputs": [],
   "source": [
    "Z = zip(output,val_output)\n",
    "\n",
    "comparison = [o == v for o,v in Z]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went right, this should return an empty list\n",
    "\n",
    "If not, inspect the elements that differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T10:12:01.089486Z",
     "start_time": "2020-12-21T10:12:01.085580Z"
    }
   },
   "outputs": [],
   "source": [
    "np.where(comparison == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = data_dir + \"bart_output.txt\"\n",
    "\n",
    "with open(output_path,'w') as f:\n",
    "    for l in output:\n",
    "        f.write(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
