{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T04:59:47.503164Z",
     "start_time": "2020-12-21T04:59:47.491446Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "from transformers import Trainer,TrainingArguments,BartConfig,BartTokenizer,BartModel,BartForConditionalGeneration\n",
    "from transformers import BartTokenizerFast\n",
    "from torch.utils import data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"big_data\" contains precomputed simulations for 198 length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T05:00:57.807216Z",
     "start_time": "2020-12-21T05:00:57.803156Z"
    }
   },
   "outputs": [],
   "source": [
    "big_data = \"NP_sim_output198_1000000.txt\"\n",
    "big_data_header = [\"dna\",\"length\",\"energy\",\"struct\",\"blank\",\"prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T05:01:11.571715Z",
     "start_time": "2020-12-21T05:01:11.564879Z"
    }
   },
   "source": [
    "\"small_data\" is only of length 32, with fewer sequences and is more managable\n",
    "\n",
    "Future work will be on a more flexible model that can handle variable lengths, but current intended use is only on sequences of a known set length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T05:01:15.115965Z",
     "start_time": "2020-12-21T05:01:15.112059Z"
    }
   },
   "outputs": [],
   "source": [
    "small_data = \"dna_small_sim_output.txt\"\n",
    "small_data_header = [\"dna\",\"length\",\"energy\",\"struct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load prefered data set into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T07:09:36.839605Z",
     "start_time": "2020-12-21T07:09:36.520275Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shoshi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "header_names = small_data_header\n",
    "data = pd.read_csv(small_data,sep=\"\\\\t\",header=None,names=header_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the DNA and structure up for the tokenizer \n",
    "\n",
    "This is because the tokenizer expects to see words, not individual characters \n",
    "\n",
    "(it was made originally for NLP)\n",
    "\n",
    "(building custom tokenizer may be possible, but model was prebuilt to be used with this tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T07:09:39.692284Z",
     "start_time": "2020-12-21T07:09:39.283638Z"
    }
   },
   "outputs": [],
   "source": [
    "dna = data[\"dna\"]\n",
    "struct = data[\"struct\"]\n",
    "\n",
    "dna_list = [\" \".join(list(d)) for d in data[\"dna\"]]\n",
    "struct_list = [\" \".join(list(s)) for s in data[\"struct\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T07:09:39.825595Z",
     "start_time": "2020-12-21T07:09:39.820711Z"
    }
   },
   "outputs": [],
   "source": [
    "length=len(dna[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using same tokenizer model was trained for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:44:03.788805Z",
     "start_time": "2020-12-20T21:44:01.419476Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:44:17.341982Z",
     "start_time": "2020-12-20T21:44:17.326968Z"
    }
   },
   "source": [
    "Load model from trainer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T07:09:47.356425Z",
     "start_time": "2020-12-21T07:09:46.737653Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('.\\DNA_BART_32', return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will translate individual sequences or batches of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T07:09:49.122085Z",
     "start_time": "2020-12-21T07:09:49.103984Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate(dna,model=model,tokenizer=tokenizer,max_length=length+2):#+2 is because models are weird\n",
    "    # tokenize input\n",
    "    inputTensor = tokenizer(dna,return_tensors='pt')\n",
    "    # generate output tensor\n",
    "    outputTensor = model.generate(inputTensor['input_ids'],\n",
    "                                 max_length = max_length,\n",
    "                                  early_stopping=True)\n",
    "    # decode and clean up output\n",
    "    output = [\"\".join(tokenizer.decode(t[2:-1].tolist()).split()) for t in outputTensor]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T05:21:07.865154Z",
     "start_time": "2020-12-21T05:21:07.860271Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(dna_list[:5],return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-21T07:10:54.207Z"
    }
   },
   "outputs": [],
   "source": [
    "output = [translate(d) for d in dna_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T07:03:49.597849Z",
     "start_time": "2020-12-21T07:03:49.590507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..........................................................................(((((((((((((((((((((((((((........................................)))))))))))))))))))))))))).))...........................']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bartoutput.txt\",'w') as f:\n",
    "    for l in output:\n",
    "        f.write(l)\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
